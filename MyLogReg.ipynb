{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Моя реализация алгоритма логистической регрессии"
      ],
      "metadata": {
        "id": "MjrNUWNiaHuR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Импортируем необходимые библиотеки для создания класса и для сравнения результатов предсказания с встроенной в sklearn моделью."
      ],
      "metadata": {
        "id": "LK4V_ZnzaRd8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score"
      ],
      "metadata": {
        "id": "qyPj6Xjsfw9r"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Данная модель принимает на вход следующие параметры:\n",
        "n_iter - количество итераций алгоритма. По умолчанию: 100\n",
        "\n",
        "learning_rate - коэффициент скорости обучения градиентного спуска. По умолчанию: 0.1\n",
        "\n",
        "metric - метрика, рассчитывающаяся в процессе и в конце обучения модели. Поддерживаемые значения: 'accuracy', 'precision', 'recall', 'f1', 'roc_auc'. По умолчанию: None\n",
        "\n",
        "reg - тип регуляризации. Поддерживаемые значения: 'l1' - для L1 регуляризации, 'l2' - для L2, 'elasticnet' - для комбинации L1 и L2 регуляризаций. По умолчанию: None\n",
        "\n",
        "l1_coef - коэффициент регуляризации L1. По умолчанию: 0\n",
        "\n",
        "l2_coef - коэффициент регуляризации L2. По умолчанию: 0\n",
        "\n",
        "sgd_sample - размер выборки для обучения стохастического градиентного бустинга. Значения от 0 до 1 задают долю исходной выборки, а целые значения >1 - количество элементов исходной выборки, которое будет использоваться. По умолчанию: None\n",
        "\n",
        "random_state - зерно генератора для воспроизводимости результатов"
      ],
      "metadata": {
        "id": "WmUBODrya_-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MyLogReg поддерживает следующие методы:\n",
        "fit(X: pd.DataFrame(), y: pd.Series(), verbose: None) - обучает модель на полученных данных. Доступна встроенная регуляризация трех типов, параллельный расчет одной из пяти обозначенных метрик, вывод лога компиляции и стохастический градиентный спуск для больших исходных данных.\n",
        "\n",
        "get_coef() - возвращает коэффициенты обученной модели\n",
        "\n",
        "predict(X) - возвращает предсказанные на основе X значения y\n",
        "\n",
        "get_best_score() - возвращает заданную при инициализации метрику, рассчитанную для обученной модели"
      ],
      "metadata": {
        "id": "uKCyHx5ebpmI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "m7dfPgyadKH0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "class MyLogReg:\n",
        "    def __init__(self, n_iter=10, learning_rate=0.1, metric=None, reg=None, l1_coef=0, l2_coef=0, sgd_sample=None, random_state=42):\n",
        "        self.n_iter = n_iter\n",
        "        self.learning_rate = learning_rate\n",
        "        self.metric = metric\n",
        "        self.reg = reg\n",
        "        self.l1_coef = l1_coef\n",
        "        self.l2_coef = l2_coef\n",
        "        self.sgd_sample = sgd_sample\n",
        "        self.random_state = random_state\n",
        "        self.weights = None\n",
        "\n",
        "    def __str__(self):\n",
        "        return f'MyLogReg class: n_iter={self.n_iter}, learning_rate={self.learning_rate}'\n",
        "\n",
        "    def fit(self, X: pd.DataFrame(), y: pd.Series(), verbose=False):\n",
        "        np.random.seed(self.random_state)\n",
        "        random.seed(self.random_state)\n",
        "\n",
        "        X = np.hstack((np.ones((X.shape[0], 1)), X.values))\n",
        "        y = y.values\n",
        "        weights = np.ones(X.shape[1])\n",
        "        n_full = len(y)\n",
        "\n",
        "        if self.sgd_sample:\n",
        "            if self.sgd_sample <= 1.0:\n",
        "                n = round(n_full * self.sgd_sample)\n",
        "            else:\n",
        "                n = self.sgd_sample\n",
        "        else:\n",
        "            n = n_full\n",
        "\n",
        "        for i in range(1, self.n_iter + 1):\n",
        "            if self.sgd_sample:\n",
        "                sample_row_idx = random.sample(range(n_full), n)\n",
        "                X_sample = X[sample_row_idx, :]\n",
        "                y_sample = y[sample_row_idx]\n",
        "            else:\n",
        "                X_sample = X\n",
        "                y_sample = y\n",
        "\n",
        "            y_pred = 1 / (1 + np.exp(-np.dot(X_sample, weights)))\n",
        "            y_pred_full = 1 / (1 + np.exp(-np.dot(X, weights)))\n",
        "\n",
        "            if self.reg == 'l1':\n",
        "                log_loss = -np.mean(y * np.log(y_pred_full + 1e-15) + (1 - y) * np.log(1 - y_pred_full + 1e-15)) + self.l1_coef * np.sum(np.abs(weights))\n",
        "                gradient = np.dot(X_sample.T, (y_pred - y_sample)) / n + self.l1_coef * np.sign(weights)\n",
        "            elif self.reg == 'l2':\n",
        "                log_loss = -np.mean(y * np.log(y_pred_full + 1e-15) + (1 - y) * np.log(1 - y_pred_full + 1e-15)) + self.l2_coef * np.sum(weights ** 2)\n",
        "                gradient = np.dot(X_sample.T, (y_pred - y_sample)) / n + 2 * self.l2_coef * weights\n",
        "            elif self.reg == 'elasticnet':\n",
        "                log_loss = -np.mean(y * np.log(y_pred_full + 1e-15) + (1 - y) * np.log(1 - y_pred_full + 1e-15)) + self.l1_coef * np.sum(np.abs(weights)) + self.l2_coef * np.sum(weights ** 2)\n",
        "                gradient = np.dot(X_sample.T, (y_pred - y_sample)) / n + self.l1_coef * np.sign(weights) + 2 * self.l2_coef * weights\n",
        "            else:\n",
        "                log_loss = -np.mean(y * np.log(y_pred_full + 1e-15) + (1 - y) * np.log(1 - y_pred_full + 1e-15))\n",
        "                gradient = np.dot(X_sample.T, (y_pred - y_sample)) / n\n",
        "\n",
        "            if callable(self.learning_rate):\n",
        "                learning_rate = self.learning_rate(i)\n",
        "            else:\n",
        "                learning_rate = self.learning_rate\n",
        "\n",
        "            weights -= learning_rate * gradient\n",
        "\n",
        "            if verbose or i == self.n_iter:\n",
        "                y_pred_full = 1 / (1 + np.exp(-np.dot(X, weights)))\n",
        "                y_classes = (y_pred_full >= 0.5).astype(int)\n",
        "\n",
        "                if self.metric == 'accuracy':\n",
        "                    metric = np.mean(y_classes == y)\n",
        "                elif self.metric == 'precision':\n",
        "                    metric = np.sum((y_classes == 1) & (y_classes == y)) / np.sum(y_classes == 1)\n",
        "                elif self.metric == 'recall':\n",
        "                    metric = np.sum((y == 1) & (y_classes == y)) / np.sum(y == 1)\n",
        "                elif self.metric == 'f1':\n",
        "                    precision = np.sum((y_classes == 1) & (y_classes == y)) / np.sum(y_classes == 1)\n",
        "                    recall = np.sum((y == 1) & (y_classes == y)) / np.sum(y == 1)\n",
        "                    metric = 2 * precision * recall / (precision + recall)\n",
        "                elif self.metric == 'roc_auc':\n",
        "                    if np.sum(y == 1) == 0 or np.sum(y == 0) == 0:\n",
        "                        metric = 'ValueError'\n",
        "                    else:\n",
        "                        y_pred_sort = y_pred_full.argsort()[::-1]\n",
        "                        y_sort = y[y_pred_sort]\n",
        "                        metric = 1/np.sum(y==1)/np.sum(y==0) * np.sum([np.sum((y_sort[i]<y_sort).astype(int)*(0*(y_pred_sort[i]>y_pred_sort).astype(int) + 0.5*((y_pred_sort[i]==y_pred_sort).astype(int)) + 1*(y_pred_sort[i]<y_pred_sort).astype(int))) for i in range(len(y))])\n",
        "\n",
        "                if verbose:\n",
        "                    if self.metric:\n",
        "                        if i == 1:\n",
        "                            print(f'start | loss: {log_loss} | {self.metric}: {metric}')\n",
        "                        elif i % verbose == 0:\n",
        "                            print(f'{i} | loss: {log_loss} | {self.metric}: {metric}')\n",
        "                    else:\n",
        "                        if i == 1:\n",
        "                            print(f'start | loss: {log_loss}')\n",
        "                        elif i % verbose == 0:\n",
        "                            print(f'{i} | loss: {log_loss}')\n",
        "\n",
        "                if i == self.n_iter and metric is not None:\n",
        "                    self.metric_value = metric\n",
        "\n",
        "        self.weights = weights\n",
        "\n",
        "    def get_coef(self):\n",
        "        return self.weights[1:]\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        X = np.hstack((np.ones((X.shape[0], 1)), X.values))\n",
        "        return 1 / (1 + np.exp(-np.dot(X, self.weights)))\n",
        "\n",
        "    def predict(self, X):\n",
        "        X = np.hstack((np.ones((X.shape[0], 1)), X.values))\n",
        "        return (1 / (1 + np.exp(-np.dot(X, self.weights))) > 0.5).astype(int)\n",
        "\n",
        "    def get_best_score(self):\n",
        "        return self.metric_value"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Создаем выборку размера 10000 с помощью встроенной в sklearn функции"
      ],
      "metadata": {
        "id": "xoFd8KNWkNcn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = make_classification(n_samples=10000, n_features=14, n_informative=10, random_state=42)\n",
        "X = pd.DataFrame(X)\n",
        "y = pd.Series(y)"
      ],
      "metadata": {
        "id": "3UT0tW3IfWke"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Обучим нашу модель и спрогнозируем классы для тех же данных. Затем обучим встроенную в sklearn модель и сравним их метрики и прогнозы."
      ],
      "metadata": {
        "id": "LkNCYYQokSLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyLogReg(metric='roc_auc')\n",
        "model.fit(X, y)\n",
        "y_pred = model.predict(X)\n",
        "\n",
        "accuracy_score(y, y_pred), precision_score(y, y_pred), recall_score(y, y_pred), f1_score(y, y_pred), roc_auc_score(y, y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UTpYeBEjI9f",
        "outputId": "651c24ca-371b-4122-8bf7-576ea142b5fa"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.5284, 0.5240677966101694, 0.6184, 0.5673394495412845, 0.5284)"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_sk = LogisticRegression()\n",
        "model_sk.fit(X, y)\n",
        "y_pred_sk = model.predict(X)\n",
        "\n",
        "accuracy_score(y, y_pred_sk), precision_score(y, y_pred_sk), recall_score(y, y_pred_sk), f1_score(y, y_pred_sk), roc_auc_score(y, y_pred_sk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z20VjQn_cMQe",
        "outputId": "fa87536a-2ce8-411d-c0b5-78f3dc6c9302"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.5284, 0.5240677966101694, 0.6184, 0.5673394495412845, 0.5284)"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(y_pred==y_pred_sk).sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IImpTRJcjTAg",
        "outputId": "452a33c2-1314-4099-e1f4-1983e6b2561c"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {},
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как мы видим из результатов сравнения предсказаний, построенная модель дает тот же результат, что и существующая в библиотеке sklearn. Помимо прочего, данная реализация поддерживает расчет метрик в процессе обучения, регуляризацию и стохастический градиентный спуск, из чего я делаю вывод, что модель успешна."
      ],
      "metadata": {
        "id": "rsGvrsKAkrXe"
      }
    }
  ]
}